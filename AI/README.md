
### ğŸ“œ AIç»å…¸ç ”ç©¶è®ºæ–‡ä¸è´¡çŒ®æ¦‚è§ˆ

| é˜¶æ®µ/é¢†åŸŸ | è®ºæ–‡åç§° | å‘è¡¨å¹´ä»½ | æ ¸å¿ƒè´¡çŒ®/åœ°ä½ |
| :--- | :--- | :--- | :--- |
| **ğŸ§  æ—©æœŸåŸºç¡€ä¸ç†è®º** | Computing Machinery and Intelligence | 1950 | æå‡º**å›¾çµæµ‹è¯•**ï¼Œå®šä¹‰äº†æœºå™¨æ™ºèƒ½ã€‚ |
| | A Logical Calculus of the Ideas Immanent in Nervous Activity | 1943 | æå‡º**M-Pç¥ç»å…ƒæ¨¡å‹**ï¼Œæ˜¯ç¥ç»ç½‘ç»œçš„åŸºç¡€ã€‚ |
| | The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain | 1958 | æå‡º**æ„ŸçŸ¥æœºæ¨¡å‹**ï¼Œæ€èµ·äº†ç¬¬ä¸€æ¬¡ç¥ç»ç½‘ç»œç ”ç©¶çƒ­æ½®ã€‚ |
| | Learning representations by back-propagating errors | 1986 | ç³»ç»Ÿé˜è¿°äº†**åå‘ä¼ æ’­ç®—æ³•**ï¼ˆBPç®—æ³•ï¼‰ï¼Œæ˜¯è®­ç»ƒæ·±åº¦ç½‘ç»œçš„åŸºçŸ³ã€‚ |
| **ğŸš€ æ·±åº¦å­¦ä¹ å…´èµ·** | ImageNet Classification with Deep Convolutional Neural Networks | 2012 | **AlexNet**åœ¨ImageNetå¤§èµ›å¤ºå† ï¼Œå¼€å¯äº†ç°ä»£æ·±åº¦å­¦ä¹ æ—¶ä»£ã€‚ |
| | Visualizing and Understanding Convolutional Networks | 2014 | æå‡ºäº†**ZFNet**ï¼Œå¹¶é€šè¿‡å¯è§†åŒ–æ·±å…¥è§£é‡Šäº†CNNçš„å·¥ä½œåŸç†ã€‚ |
| **è®¡ç®—æœºè§†è§‰** | Deep Residual Learning for Image Recognition | 2015 | æå‡º**ResNetï¼ˆæ®‹å·®ç½‘ç»œï¼‰**ï¼Œé€šè¿‡è·³è·ƒè¿æ¥æˆåŠŸè®­ç»ƒäº†ææ·±çš„ç½‘ç»œã€‚ |
| | You Only Look Once: Unified, Real-Time Object Detection | 2016 | æå‡º**YOLO** ç›®æ ‡æ£€æµ‹ç®—æ³•ï¼Œå®ç°äº†ç²¾åº¦ä¸é€Ÿåº¦çš„å¹³è¡¡ã€‚ |
| **ä¼˜åŒ–ä¸è®­ç»ƒæŠ€å·§** | Dropout: A Simple Way to Prevent Neural Networks from Overfitting | 2014 | æå‡º**Dropout**æ­£åˆ™åŒ–æ–¹æ³•ï¼Œæœ‰æ•ˆé˜²æ­¢è¿‡æ‹Ÿåˆã€‚ |
| | Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift | 2015 | æå‡º**æ‰¹é‡å½’ä¸€åŒ–ï¼ˆBatch Normï¼‰**ï¼Œå¤§å¹…åŠ é€Ÿæ·±åº¦ç½‘ç»œè®­ç»ƒã€‚ |
| | Adam: A Method for Stochastic Optimization | 2014 | æå‡º**Adamä¼˜åŒ–å™¨**ï¼Œæˆä¸ºæœ€å¹¿æ³›ä½¿ç”¨çš„è‡ªé€‚åº”ä¼˜åŒ–ç®—æ³•ä¹‹ä¸€ã€‚ |
| **ç”Ÿæˆæ¨¡å‹** | Generative Adversarial Nets | 2014 | æå‡º**ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰**ï¼Œå¼€å¯äº†ç”Ÿæˆå¼æ¨¡å‹çš„æ–°èŒƒå¼ã€‚ |
| | Auto-Encoding Variational Bayes | 2013 | æå‡º**å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰**ï¼Œæ˜¯é‡è¦çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹ã€‚ |
| **ğŸ’¬ å¤§è¯­è¨€æ¨¡å‹æ—¶ä»£** | Attention Is All You Need | 2017 | æå‡º**Transformer**æ¶æ„ï¼Œå®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œæˆä¸ºç°ä»£å¤§æ¨¡å‹çš„æ ¸å¿ƒã€‚ |
| | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | 2018 | æå‡º**BERT**æ¨¡å‹ï¼Œé€šè¿‡åŒå‘é¢„è®­ç»ƒåœ¨å¤šé¡¹NLPä»»åŠ¡ä¸Šå–å¾—çªç ´ã€‚ |
| | Improving Language Understanding by Generative Pre-Training (**GPT-1**) | 2018 | æå‡º**GPTç³»åˆ—**çš„åˆä»£æ¨¡å‹ï¼Œå¼€åˆ›äº†ç”Ÿæˆå¼é¢„è®­ç»ƒè·¯å¾„ã€‚ |
| | Language Models are Few-Shot Learners (**GPT-3**) | 2020 | æå‡ºæ‹¥æœ‰1750äº¿å‚æ•°çš„**GPT-3**ï¼Œå±•ç¤ºäº†**å¤§æ¨¡å‹çš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›**ã€‚ |
| | Training language models to follow instructions with human feedback (**InstructGPT**) | 2022 | æå‡ºåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ï¼Œè®©æ¨¡å‹è¾“å‡ºä¸äººç±»æ„å›¾å¯¹é½ã€‚ |
| | LLaMA: Open and Efficient Foundation Language Models | 2023 | Metaå‘å¸ƒçš„å¼€æºé«˜æ•ˆå¤§æ¨¡å‹ç³»åˆ—ï¼Œæ¨åŠ¨äº†å¼€æºç¤¾åŒºå‘å±•ã€‚ |
